import torch.distributed as dist
import torch.multiprocessing as mp
from torch.utils.data.distributed import DistributedSampler
from torch.utils.data import DataLoader

from dataset import dataset_registry
from dataset.loader import MetaLoader, PrefetchLoader
from utils.distributed import DistributedSampler_wopadding
from utils.logger import LOGGER


def create_train_dataloaders(final_config, device):
    data_cfg = final_config.data_cfg.train
    dataloaders = []
    dataloaders_dict = {}
    train_steps = []
    loader_names = []

    if len(data_cfg) == 0:
        return

    for d_cfg in data_cfg:
        name = d_cfg['name']
        dataset = dataset_registry[d_cfg.type](d_cfg, final_config, device)

        collate_fn = dataset.collate_fn
        worker_init_fn = dataset.worker_init_fn
        use_sampler = dataset.use_sampler

        LOGGER.info("Create Dataset {} Success".format(name))
        task = d_cfg['task']
        batch_size = d_cfg['batch_size']
        n_workers = d_cfg['n_workers']

        if 'steps' in d_cfg:
            train_steps.append(d_cfg['steps'])
        elif 'epoch' in d_cfg:
            epoch = d_cfg['epoch']
            train_steps.append(int((len(dataset) // batch_size) * epoch))

        loader = build_dataloader(dataset, collate_fn, True,
                                  batch_size // final_config.run_cfg.gradient_accumulation_steps, n_workers=n_workers,
                                  worker_init_fn=worker_init_fn, use_sampler=use_sampler)

        dataloaders.append(loader)
        loader_names.append(f'{task}--{name}')

    for i in range(len(dataloaders)):
        ratio = train_steps[i]
        dataloaders_dict[loader_names[i]] = (dataloaders[i], ratio)

    n_gpu = dist.get_world_size()
    for name, (loader, ratio) in dataloaders_dict.items():
        LOGGER.info(f" loader {name} , ratio {ratio} , bs_per_gpu {loader.batch_size}, n_workers {loader.num_workers}")

    meta_loader = MetaLoader(dataloaders_dict,
                             accum_steps=final_config.run_cfg.gradient_accumulation_steps,
                             distributed=n_gpu > 1)

    if final_config.run_cfg.num_train_steps == 0:
        total_train_steps = sum(train_steps)
        final_config.run_cfg.num_train_steps = total_train_steps

    meta_loader = PrefetchLoader(meta_loader, device)
    meta_loader.ndata = len(dataloaders_dict)
    final_config.run_cfg.valid_steps = final_config.run_cfg.num_train_steps // final_config.run_cfg.valid_freq - 1
    return meta_loader


def create_val_dataloaders(final_config, device):
    data_cfg = final_config.data_cfg.val
    dataloaders = {}
    for d_cfg in data_cfg:
        name = d_cfg['name']
        dataset = dataset_registry[d_cfg.type](d_cfg, final_config, device)
        collate_fn = dataset.collate_fn
        worker_init_fn = dataset.worker_init_fn
        use_sampler = dataset.use_sampler
        dataset.name = name
        LOGGER.info("Create Dataset {} Success. Len {}".format(name, len(dataset)))
        task = d_cfg['task']
        batch_size = d_cfg['batch_size']
        n_workers = d_cfg['n_workers']
        loader = build_dataloader(dataset, collate_fn, False, batch_size, n_workers=n_workers,
                                  worker_init_fn=worker_init_fn, use_sampler=use_sampler)
        task_name = f'{task}--{name}'
        dataloaders[task_name] = PrefetchLoader(loader, device)
    return dataloaders


def build_dataloader(dataset, collate_fn, is_train, batch_size, n_workers=None, worker_init_fn=None, use_sampler=True):
    batch_size = batch_size // dist.get_world_size()
    ctx = mp.get_context("spawn")
    if use_sampler:
        if is_train:
            sampler = DistributedSampler(dataset, shuffle=True)
        else:
            sampler = DistributedSampler_wopadding(dataset)
        if is_train:
            loader = DataLoader(dataset, sampler=sampler, batch_size=batch_size,
                                num_workers=n_workers, pin_memory=True,
                                collate_fn=collate_fn, drop_last=is_train,
                                worker_init_fn=worker_init_fn, multiprocessing_context=ctx,
                                persistent_workers=True)
        else:
            loader = DataLoader(dataset, sampler=sampler, batch_size=batch_size,
                                num_workers=n_workers, pin_memory=True,
                                collate_fn=collate_fn, drop_last=is_train, worker_init_fn=worker_init_fn,
                                multiprocessing_context=ctx, persistent_workers=True)
    else:
        if is_train:
            loader = DataLoader(dataset, batch_size=batch_size,
                                num_workers=n_workers, pin_memory=True,
                                collate_fn=collate_fn, drop_last=is_train,
                                worker_init_fn=worker_init_fn, multiprocessing_context=ctx,
                                persistent_workers=True)
        else:
            loader = DataLoader(dataset, batch_size=batch_size,
                                num_workers=n_workers, pin_memory=True,
                                collate_fn=collate_fn, drop_last=is_train, worker_init_fn=worker_init_fn,
                                multiprocessing_context=ctx, persistent_workers=True)
    return loader
